{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"./img/cours_3_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"./img/SV_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_6.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"./img/SV_7.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn import svm\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = svm.SVC(kernel='linear')\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_8.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_9.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn import svm\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_6.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_7.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_8.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'max_depth': 4, 'min_samples_split': 10}\n",
      "\n",
      "Score = \n",
      " 0.7270538693786748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Noms des colonnes correspondant aux caractéristiques et à la cible\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "target_col = 'WHODASTTB'\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.6262553802008608\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)\n",
    "\n",
    "# On construit une image d'un arbre de décisions\n",
    "tree.export_graphviz(\n",
    "            model,\n",
    "            out_file = './img/_tree.dot',\n",
    "            feature_names = features_cols,\n",
    "            class_names = target_col,\n",
    "            filled = True,\n",
    "            rounded = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_9.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'max_depth': 4, 'max_features': 7, 'min_samples_split': 2, 'n_estimators': 5}\n",
      "\n",
      "Score = \n",
      " 0.7453678690608613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=4, max_features=7, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "target_col = 'WHODASTTB'\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'n_estimators':[5], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.68974175035868\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)\n",
    "\n",
    "# On construit une image d'un arbre de décisions\n",
    "tree.export_graphviz(\n",
    "            model[0],\n",
    "            out_file = './img/_tree.dot',\n",
    "            feature_names = features_cols,\n",
    "            class_names = target_col,\n",
    "            filled = True,\n",
    "            rounded = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_6.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_7.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_8.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_9.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_10.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_11.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 10, 'n_estimators': 5}\n",
      "\n",
      "Score = \n",
      " 0.7540402034006038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=4,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=10,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "target_col = 'WHODASTTB'\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'learning_rate':[0.1, 0.2, 0.4, 0.8], \\\n",
    "                      'n_estimators':[5], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.665351506456241\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)\n",
    "\n",
    "# On construit une image d'un arbre de décisions\n",
    "tree.export_graphviz(\n",
    "            model[1, 0],\n",
    "            out_file = './img/_tree.dot',\n",
    "            feature_names = features_cols,\n",
    "            class_names = target_col,\n",
    "            filled = True,\n",
    "            rounded = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./img/NB_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'var_smoothing': 1e-08}\n",
      "\n",
      "Score = \n",
      " 0.7162084856189417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-08)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = GaussianNB()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'var_smoothing':[1e-8, 1e-9, 1e-10]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.5893113342898135\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/KN_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/KN_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'n_neighbors': 16, 'weights': 'distance'}\n",
      "\n",
      "Score = \n",
      " 0.6926744001271252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=16, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'n_neighbors':[1, 2, 4, 8, 16], 'weights':['uniform', 'distance']}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.706599713055954\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Recap_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Recap_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Recap_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Select_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXEMPLE \"FILTRAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 9)\n",
      "(297, 9)\n",
      "Data ready\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'max_depth': 4, 'min_samples_split': 10}\n",
      "\n",
      "Score = \n",
      " 0.7270538693786748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction de sélection de données\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Noms des colonnes correspondant aux caractéristiques et à la cible\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "target_col = 'WHODASTTB'\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X_train)\n",
    "sel.fit_transform(X_test)\n",
    "print(X_train.shape)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.6262553802008608\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXEMPLE \"WRAPPER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 9)\n",
      "(297, 9)\n",
      "Data ready\n",
      "\n",
      "\n",
      "ITÉRATION 1\n",
      "\n",
      "\n",
      "Nombre de caractéristiques :\n",
      " 9\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'max_depth': 4, 'min_samples_split': 10}\n",
      "\n",
      "Score = \n",
      " 0.7270538693786748\n",
      "\n",
      "Importance des caractéristiques : \n",
      " [0.0255661  0.52284761 0.         0.04807339 0.11804509 0.1492882\n",
      " 0.1361796  0.         0.        ]\n",
      "(297, 9)\n",
      "(297, 3)\n",
      "\n",
      "\n",
      "ITÉRATION 2\n",
      "\n",
      "\n",
      "Nombre de caractéristiques :\n",
      " 3\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'max_depth': 2, 'min_samples_split': 2}\n",
      "\n",
      "Score = \n",
      " 0.7329334180835849\n",
      "\n",
      "Importance des caractéristiques : \n",
      " [0.86004273 0.13995727 0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction de sélection de données\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Noms des colonnes correspondant aux caractéristiques et à la cible\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "target_col = 'WHODASTTB'\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X_train)\n",
    "sel.fit_transform(X_test)\n",
    "print(X_train.shape)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser une première itération de la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "print('\\n\\nITÉRATION 1\\n')\n",
    "print('\\nNombre de caractéristiques :\\n', X_train.shape[1])\n",
    "# Réaliser une validation croisée avec grille de recherche pour les hyperparamètres\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "print('\\nImportance des caractéristiques : \\n', model.feature_importances_)\n",
    "\n",
    "print(X_train.shape)\n",
    "# Sélectionner les caractéristiques les plus importantes\n",
    "features_new = SelectFromModel(model, threshold=0.1, prefit=True, max_features=3)\n",
    "X_train = features_new.transform(X_train)\n",
    "X_test = features_new.transform(X_test)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Réaliser une deuxième itération de la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "print('\\n\\nITÉRATION 2\\n')\n",
    "print('\\nNombre de caractéristiques :\\n', X_train.shape[1])\n",
    "# Réaliser une nouvelle validation croisée avec grille de recherche pour les hyperparamètres\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "print('\\nImportance des caractéristiques : \\n', model.feature_importances_)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.6603299856527978\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXEMPLE \"INTÉGRÉE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "Meilleurs hyperparamètres: \n",
      " {'C': 0.1}\n",
      "\n",
      "Score = \n",
      " 0.7475011918004131\n",
      "\n",
      "Importance des caractéristiques : \n",
      " [[0.         0.80137513 0.         0.         0.         0.00362856\n",
      "  0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Colonnes correspondant à des caractéristiques\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "\n",
    "# Données importées (X: caractéristiques, y: cible)\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "# Séparation en données d'entraînement et en données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Standatdisation des entrées\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(penalty='l1')\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Définir les plus de la validation croisée\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Définir le type de score utilisé pour sélectionner les hyperparamètres dans la validation croisée\n",
    "scoring='roc_auc'\n",
    "\n",
    "# Réaliser la validation croisée avec grille de recherche pour les hyperparamètres.\n",
    "cv_valid = GridSearchCV(estimator=model, param_grid=hyperparams, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "model = cv_valid.best_estimator_\n",
    "print('\\nMeilleurs hyperparamètres: \\n', best_params)\n",
    "print('\\nScore = \\n', best_score)\n",
    "print('\\nImportance des caractéristiques : \\n', model.coef_)\n",
    "\n",
    "# Entraîner le modèle final avec toutes les données d'entraînement\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.6406025824964132\n"
     ]
    }
   ],
   "source": [
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/MC_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/MC_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/MC_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/MC_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/MC_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "logistic\n",
      "\n",
      "Meilleur paramètre:  {'C': 1000}\n",
      "\n",
      "Score =  0.272270737503952\n",
      "\n",
      "SVC_linear\n",
      "\n",
      "Meilleur paramètre:  {'C': 10}\n",
      "\n",
      "Score =  0.27650306407984937\n",
      "\n",
      "SVC_rbf\n",
      "\n",
      "Meilleur paramètre:  {'C': 1000, 'gamma': 1}\n",
      "\n",
      "Score =  0.3809556829162604\n",
      "\n",
      "tree\n",
      "\n",
      "Meilleur paramètre:  {'max_depth': 12, 'min_samples_split': 6}\n",
      "\n",
      "Score =  0.28160113589197633\n",
      "\n",
      "forest\n",
      "\n",
      "Meilleur paramètre:  {'max_depth': 8, 'max_features': 9, 'min_samples_split': 2, 'n_estimators': 5}\n",
      "\n",
      "Score =  0.36359885148800425\n",
      "\n",
      "naive bayes\n",
      "\n",
      "Meilleur paramètre:  {'alpha': 1}\n",
      "\n",
      "Score =  0.20031841689265756\n",
      "\n",
      "knn\n",
      "\n",
      "Meilleur paramètre:  {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "\n",
      "Score =  0.43204561551884185\n",
      "\n",
      "\n",
      " Analyse terminée\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "# Retirer les lignes comportant des données manquantes\n",
    "data = data.dropna()\n",
    "# Retirer les sujets contrôles\n",
    "data[data.CONTROL != 1]\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['CIMDX']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(multi_class='ovr')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "model_rbf = svm.SVC(kernel='rbf')\n",
    "model_tree = DecisionTreeClassifier()\n",
    "model_forest = RandomForestClassifier()\n",
    "model_boosting = GradientBoostingClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_knn = KNeighborsClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_rbf = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_tree = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "hyperparams_forest = {'n_estimators':[5], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "hyperparams_boosting = {'learning_rate':[0.1, 0.2, 0.4, 0.8], \\\n",
    "                      'n_estimators':[5], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "hyperparams_nb = {'alpha':[1]}\n",
    "hyperparams_knn = {'n_neighbors':[1, 2, 4, 8, 16], 'weights':['uniform', 'distance']}\n",
    "\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "scoring = make_scorer(f1_score , average='macro')\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_rbf, param_grid=hyperparams_rbf, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_rbf')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_tree, param_grid=hyperparams_tree, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\ntree')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_forest, param_grid=hyperparams_forest, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nforest')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\"\"\"\n",
    "cv_valid = GridSearchCV(estimator=model_boosting, param_grid=hyperparams_boosting, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nboosting')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\"\"\"\n",
    "cv_valid = GridSearchCV(estimator=model_nb, param_grid=hyperparams_nb, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nnaive bayes')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_knn, param_grid=hyperparams_knn, cv=cv_folds, scoring=scoring)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nknn')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "print(\"\\n\\n Analyse terminée\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
