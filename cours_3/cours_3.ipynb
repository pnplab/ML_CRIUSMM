{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"./img/cours_3_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"./img/SV_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_6.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <img src=\"./img/SV_7.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "logistic\n",
      "\n",
      "Meilleur paramètre:  {'C': 10}\n",
      "\n",
      "Score =  0.7282686089984275\n",
      "\n",
      "SVC_linear\n",
      "\n",
      "Meilleur paramètre:  {'C': 1}\n",
      "\n",
      "Score =  0.7318297511312218\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "#features_cols = ['AUDITTT', 'STAIYTT']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(solver='lbfgs')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "scoring='roc_auc'\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_logistic, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.6456241032998566\n"
     ]
    }
   ],
   "source": [
    "# On spécifie l'algorithme final.\n",
    "#model_final = LogisticRegression(solver='lbfgs', C=.001)\n",
    "model_final = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "# On entrapine l'algorithme final.\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_8.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " <img src=\"./img/SV_9.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "logistic\n",
      "\n",
      "Meilleur paramètre:  {'C': 0.1}\n",
      "\n",
      "Score =  0.7306131194976627\n",
      "\n",
      "SVC_linear\n",
      "\n",
      "Meilleur paramètre:  {'C': 0.1}\n",
      "\n",
      "Score =  0.7307170822502487\n",
      "\n",
      "SVC_rbf\n",
      "\n",
      "Meilleur paramètre:  {'C': 10, 'gamma': 0.001}\n",
      "\n",
      "Score =  0.7365984991923664\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "#features_cols = ['AUDITTT', 'STAIYTT']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(solver='lbfgs')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "model_rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "hyperparams_rbf = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "scoring='roc_auc'\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_rbf, param_grid=hyperparams_rbf, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_rbf')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC =  0.6578192252510762\n"
     ]
    }
   ],
   "source": [
    "# On spécifie l'algorithme final.\n",
    "model_final = svm.SVC(kernel='rbf', C=10, gamma=0.001)\n",
    "#model_final = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# On entrapine l'algorithme final.\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./img/AD_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_6.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_7.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_8.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "#features_cols = ['AUDITTT', 'STAIYTT']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(solver='lbfgs')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "model_rbf = svm.SVC(kernel='rbf')\n",
    "model_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_rbf = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_tree = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "scoring='roc_auc'\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_rbf, param_grid=hyperparams_rbf, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_rbf')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_tree, param_grid=hyperparams_tree, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\ntree')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On spécifie l'algorithme final.\n",
    "#model_final = LogisticRegression(solver='lbfgs')\n",
    "#model_final = svm.SVC(kernel='linear', C=0.0001) \n",
    "#model_final = svm.SVC(kernel='rbf', C=0.1, gamma=0.1) \n",
    "model_final = DecisionTreeClassifier(max_depth=4, min_samples_split=2)\n",
    "\n",
    "# On entrapine l'algorithme final.\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/AD_9.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FA_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "#features_cols = ['AUDITTT', 'STAIYTT']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(solver='lbfgs')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "model_rbf = svm.SVC(kernel='rbf')\n",
    "model_tree = DecisionTreeClassifier()\n",
    "model_forest = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_rbf = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_tree = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "hyperparams_forest = {'n_estimators':[5, 10, 20, 50], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "scoring='roc_auc'\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_rbf, param_grid=hyperparams_rbf, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_rbf')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_tree, param_grid=hyperparams_tree, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\ntree')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_forest, param_grid=hyperparams_forest, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nforest')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On spécifie l'algorithme final.\n",
    "#model_final = LogisticRegression(solver='lbfgs')\n",
    "#model_final = svm.SVC(kernel='linear', C=0.0001) \n",
    "#model_final = svm.SVC(kernel='rbf', C=0.1, gamma=0.1) \n",
    "#model_final = DecisionTreeClassifier(max_depth=4, min_samples_split=2)\n",
    "model_final = RandomForestClassifier(n_estimators=10, max_depth=4, min_samples_split=2, max_features=3)\n",
    "\n",
    "\n",
    "# On entrapine l'algorithme final.\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_5.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_6.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_7.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_8.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_9.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_10.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/BG_11.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "forest\n",
      "\n",
      "Meilleur paramètre:  {'learning_rate': 0.1, 'max_depth': 4, 'max_features': 7, 'min_samples_split': 12, 'n_estimators': 10}\n",
      "\n",
      "Score f1 =  0.7447444450275084\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "#features_cols = ['AUDITTT', 'STAIYTT']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(solver='lbfgs')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "model_rbf = svm.SVC(kernel='rbf')\n",
    "model_tree = DecisionTreeClassifier()\n",
    "model_forest = RandomForestClassifier()\n",
    "model_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_rbf = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_tree = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "hyperparams_forest = {'n_estimators':[5, 10, 20, 50], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "hyperparams_boosting = {'learning_rate':[0.1, 0.2, 0.4, 0.8], \\\n",
    "                      'n_estimators':[10, 50], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "scoring='roc_auc'\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_rbf, param_grid=hyperparams_rbf, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_rbf')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_tree, param_grid=hyperparams_tree, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\ntree')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_forest, param_grid=hyperparams_forest, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nforest')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_boosting, param_grid=hyperparams_boosting, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nboosting')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test f1 =  0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "# On spécifie l'algorithme final.\n",
    "#model_final = LogisticRegression(solver='lbfgs')\n",
    "#model_final = svm.SVC(kernel='linear', C=0.0001) \n",
    "#model_final = svm.SVC(kernel='rbf', C=0.1, gamma=0.1) \n",
    "#model_final = DecisionTreeClassifier(max_depth=4, min_samples_split=2)\n",
    "#model_final = RandomForestClassifier(n_estimators=10, max_depth=4, min_samples_split=2, max_features=3)\n",
    "model_final = GradientBoostingClassifier(learning_rate=0.1, n_estimators=10, max_depth=4, min_samples_split=12, max_features=7)\n",
    "\n",
    "\n",
    "# On entrapine l'algorithme final.\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./img/NB_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_2.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_3.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/NB_4.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "SVC_logistic\n",
      "\n",
      "Meilleur paramètre:  {'C': 0.1}\n",
      "\n",
      "Score f1 =  0.693613483385213\n",
      "\n",
      "SVC_linear\n",
      "\n",
      "Meilleur paramètre:  {'C': 0.0001}\n",
      "\n",
      "Score f1 =  0.685860805860806\n",
      "\n",
      "SVC_rbf\n",
      "\n",
      "Meilleur paramètre:  {'C': 10, 'gamma': 100}\n",
      "\n",
      "Score f1 =  0.6906400840108706\n",
      "\n",
      "tree\n",
      "\n",
      "Meilleur paramètre:  {'max_depth': 4, 'min_samples_split': 2}\n",
      "\n",
      "Score f1 =  0.7467014001153369\n",
      "\n",
      "forest\n",
      "\n",
      "Meilleur paramètre:  {'max_depth': 4, 'max_features': 3, 'min_samples_split': 4, 'n_estimators': 10}\n",
      "\n",
      "Score f1 =  0.7301518350341623\n",
      "\n",
      "forest\n",
      "\n",
      "Meilleur paramètre:  {'learning_rate': 0.1, 'max_depth': 4, 'max_features': 7, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "\n",
      "Score f1 =  0.7336515712633288\n",
      "\n",
      "naive bayes\n",
      "\n",
      "Meilleur paramètre:  {'var_smoothing': 1e-08}\n",
      "\n",
      "Score f1 =  0.6287724332779859\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "#features_cols = ['AUDITTT', 'STAIYTT']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(solver='lbfgs')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "model_rbf = svm.SVC(kernel='rbf')\n",
    "model_tree = DecisionTreeClassifier()\n",
    "model_forest = RandomForestClassifier()\n",
    "model_boosting = GradientBoostingClassifier()\n",
    "model_NB = GaussianNB()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_rbf = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_tree = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "hyperparams_forest = {'n_estimators':[5, 10, 15, 20], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "hyperparams_boosting = {'learning_rate':[0.1, 0.2, 0.4, 0.8], \\\n",
    "                      'n_estimators':[5, 10, 15, 20], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "hyperparams_NB = {'var_smoothing':[1e-8, 1e-9, 1e-10]}\n",
    "\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "scoring='roc_auc'\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_rbf, param_grid=hyperparams_rbf, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_rbf')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_tree, param_grid=hyperparams_tree, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\ntree')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_forest, param_grid=hyperparams_forest, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nforest')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_boosting, param_grid=hyperparams_boosting, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nboosting')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_nb, param_grid=hyperparams_nb, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nnaive bayes')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test f1 =  0.5897435897435898\n"
     ]
    }
   ],
   "source": [
    "# On spécifie l'algorithme final.\n",
    "#model_final = LogisticRegression(solver='lbfgs')\n",
    "#model_final = svm.SVC(kernel='linear', C=0.0001) \n",
    "#model_final = svm.SVC(kernel='rbf', C=0.1, gamma=0.1) \n",
    "#model_final = DecisionTreeClassifier(max_depth=4, min_samples_split=2)\n",
    "#model_final = RandomForestClassifier(n_estimators=10, max_depth=4, min_samples_split=2, max_features=3)\n",
    "#model_final = GradientBoostingClassifier(learning_rate=0.1, n_estimators=10, max_depth=4, min_samples_split=12, max_features=7)\n",
    "model_final = GaussianNB(var_smoothing=1e-8)\n",
    "\n",
    "# On entrapine l'algorithme final.\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/KN_0.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/KN_1.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready\n",
      "\n",
      "naive bayes\n",
      "\n",
      "Meilleur paramètre:  {'n_neighbors': 1, 'weights': 'uniform'}\n",
      "\n",
      "Score f1 =  0.6394035480424828\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 1 : importer les librairies utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Importer les librairies utiles pour l'analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 2 : importer les fonctions utiles\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importer les fonctions de prétraitement\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importer une fonction qui nous permette de construire aléatoirement les ensembles \"Entraînement\" et \"Test\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer le modèle de régression logistique de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Importer la fonction de validation croisée\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importer la fonction permettant d'afficher le rapport de classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 3 : importer et préparer le jeu de données \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Importons un ensemble de données\n",
    "data = pd.read_csv('../data/sim_data_signature_small.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "features_cols = ['PSQ_SS', 'PHQ9TT', 'CEVQOTT', 'DAST10TT', 'AUDITTT', 'STAIYTT', 'AGE', 'SEXE', 'SES']\n",
    "#features_cols = ['AUDITTT', 'STAIYTT']\n",
    "\n",
    "X = data.loc[:, features_cols]\n",
    "y = data['WHODASTTB']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print('Data ready')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ÉTAPE 4 : définir et entraîner le modèle\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Définir le modèle\n",
    "model_logistic = LogisticRegression(solver='lbfgs')\n",
    "model_linear = svm.SVC(kernel='linear')\n",
    "model_rbf = svm.SVC(kernel='rbf')\n",
    "model_tree = DecisionTreeClassifier()\n",
    "model_forest = RandomForestClassifier()\n",
    "model_boosting = GradientBoostingClassifier()\n",
    "model_nb = MultinomialNB()\n",
    "model_knn = KNeighborsClassifier()\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "hyperparams_logistic = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_linear = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_rbf = {'C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000], 'gamma':[.0001, .001, .01, .1, 1, 10, 100, 1000, 1000]}\n",
    "hyperparams_tree = {'max_depth':[1, 2, 4, 8, 12, 16], 'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16]}\n",
    "hyperparams_forest = {'n_estimators':[5, 10, 15, 20], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "hyperparams_boosting = {'learning_rate':[0.1, 0.2, 0.4, 0.8], \\\n",
    "                      'n_estimators':[5, 10, 15, 20], \\\n",
    "                      'max_depth':[1, 2, 4, 8, 12, 16], \\\n",
    "                      'min_samples_split':[2, 4, 6, 8, 10, 12, 14, 16], \\\n",
    "                      'max_features': [1, 3, 5, 7, 9]}\n",
    "hyperparams_nb = {'var_smoothing':[1e-8, 1e-9, 1e-10]}\n",
    "hyperparams_knn = {'n_neighbors':[1, 2, 4, 8, 16], 'weights':['uniform', 'distance']}\n",
    "\n",
    "\n",
    "cv_folds = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "scoring='roc_auc'\n",
    "\n",
    "\"\"\"\n",
    "cv_valid = GridSearchCV(estimator=model_logistic, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nlogistic')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_linear, param_grid=hyperparams_linear, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_linear')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_rbf, param_grid=hyperparams_rbf, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nSVC_rbf')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_tree, param_grid=hyperparams_tree, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\ntree')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_forest, param_grid=hyperparams_forest, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nforest')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_boosting, param_grid=hyperparams_boosting, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nboosting')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\n",
    "cv_valid = GridSearchCV(estimator=model_nb, param_grid=hyperparams_nb, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nnaive bayes')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n",
    "\"\"\"\n",
    "cv_valid = GridSearchCV(estimator=model_knn, param_grid=hyperparams_knn, cv=cv_folds, scoring=scoring, iid=False)\n",
    "cv_valid.fit(X_train, y_train)\n",
    "best_params = cv_valid.best_params_\n",
    "best_score = cv_valid.best_score_\n",
    "print('\\nknn')\n",
    "print('\\nMeilleur paramètre: ', best_params)\n",
    "print('\\nScore = ', best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test f1 =  0.5915492957746479\n"
     ]
    }
   ],
   "source": [
    "# On spécifie l'algorithme final.\n",
    "#model_final = LogisticRegression(solver='lbfgs')\n",
    "#model_final = svm.SVC(kernel='linear', C=0.0001) \n",
    "#model_final = svm.SVC(kernel='rbf', C=0.1, gamma=0.1) \n",
    "#model_final = DecisionTreeClassifier(max_depth=4, min_samples_split=2)\n",
    "#model_final = RandomForestClassifier(n_estimators=10, max_depth=4, min_samples_split=2, max_features=3)\n",
    "#model_final = GradientBoostingClassifier(learning_rate=0.1, n_estimators=10, max_depth=4, min_samples_split=12, max_features=7)\n",
    "#model_final = GaussianNB(var_smoothing=1e-8)\n",
    "model_final = KNeighborsClassifier(n_neighbors=1, weights='uniform')\n",
    "\n",
    "\n",
    "# On entrapine l'algorithme final.\n",
    "model_final.fit(X_train, y_train)\n",
    "\n",
    "# On teste l'algorithme final en prédisant de nouvelles données.\n",
    "y_pred = model_final.predict(X_test)\n",
    "\n",
    "# On évalue les prédictions de l'algorithme final.\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# On affiche le résultat\n",
    "print('\\nTest AUC = ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Recap.svg\" alt=\"Drawing\" style=\"width: 2000px;\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
